{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "edbZsC5yjefW"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Adapted From: https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=mjJGEXShp7te"
      ],
      "metadata": {
        "id": "Uy8XS6sm8aZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n",
        "# !pip install -q datasets"
      ],
      "metadata": {
        "id": "9x6bLqhBVqjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h37lymrDuX5j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn\n",
        "from transformers import DataCollatorWithPadding\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "# from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classnames = [\"mistral\", \"rag\"]\n",
        "num_labels=len(classnames)\n",
        "batch_size = 16"
      ],
      "metadata": {
        "id": "kIq0trEtfwAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"labeled_data.pkl\", \"rb\") as f:\n",
        "  labeled_data = pickle.load(f)"
      ],
      "metadata": {
        "id": "UmMVuhjjgspw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_data = filter(lambda x: x[\"labels\"] in [\"mistral\", \"rag\"], labeled_data)"
      ],
      "metadata": {
        "id": "QIn-w2V59e2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "rx4sjAVDiUUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Model\n",
        "def encode(data):\n",
        "  text_lower = data[\"question\"].lower()\n",
        "  label = data[\"labels\"]\n",
        "  encoded_text = tokenizer(text_lower,\n",
        "                            padding='max_length',\n",
        "                            truncation=True,\n",
        "                            max_length=256,\n",
        "                            return_tensors=\"pt\"\n",
        "                            )\n",
        "  # set unlabeled to most difficult level\n",
        "  if label not in classnames:\n",
        "    label = classnames[-1]\n",
        "\n",
        "  encoded_text['labels'] = torch.LongTensor([classnames.index(label)])\n",
        "  return encoded_text\n",
        "\n",
        "# Use this for HF trainer\n",
        "def encode_aslist(data):\n",
        "  text_lower = data[\"question\"].lower()\n",
        "  label = data[\"labels\"]\n",
        "  encoded_text = tokenizer(text_lower,\n",
        "                            padding='max_length',\n",
        "                            truncation=True,\n",
        "                            max_length=128,\n",
        "                            # return_tensors=\"pt\"\n",
        "                            )\n",
        "  if label not in classnames:\n",
        "    label = classnames[-1]\n",
        "\n",
        "  encoded_text['labels'] = [classnames.index(label)]"
      ],
      "metadata": {
        "id": "_Ze0bxCEYpO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data = list(map(encode_aslist, labeled_data))"
      ],
      "metadata": {
        "id": "CP9640fji1-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoded_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pDnO8-pi6gv",
        "outputId": "99cfc62d-84bf-469c-fbaa-ba3aa5c49ea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using BERTForSequenceClassification + HF Trainer"
      ],
      "metadata": {
        "id": "edbZsC5yjefW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_accuracy(output):\n",
        "  pred = np.argmax(output.predictions, axis=1)\n",
        "  labels = output.label_ids\n",
        "  accuracy = (pred[:, np.newaxis] == labels).sum() / pred.shape[0]\n",
        "  return {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "ivJxP1lXMtcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                           problem_type=\"single_label_classification\",\n",
        "                                                           num_labels=num_labels,\n",
        "                                                           )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8jkPje19zfM",
        "outputId": "8bd3e869-90db-4985-bf87-2f93d408b13e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.base_model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "nF2ElLXPjkPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "vMafmBpIjnoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0eca98b-7aac-4dd9-de77-b6730b6a1311"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"bert-hotpotqa-classifier-2\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "w2faQkIsGaro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(42)\n",
        "train_set, val_set = torch.utils.data.random_split(list(encoded_data), [0.85, 0.15])"
      ],
      "metadata": {
        "id": "fVJc60_bm_f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=val_set,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=get_accuracy\n",
        ")"
      ],
      "metadata": {
        "id": "HDUt6U62Gk31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "cXMntJd8GwVu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "f3ebdfa3-6ad8-4f6d-b7a6-caa448607b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [270/270 02:22, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.711949</td>\n",
              "      <td>0.593333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.654993</td>\n",
              "      <td>0.646667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.608947</td>\n",
              "      <td>0.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.605878</td>\n",
              "      <td>0.726667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.611789</td>\n",
              "      <td>0.733333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=270, training_loss=0.5757288614908854, metrics={'train_runtime': 142.6393, 'train_samples_per_second': 29.795, 'train_steps_per_second': 1.893, 'total_flos': 279558006336000.0, 'train_loss': 0.5757288614908854, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Model (performed worse than default BERT Classifier)"
      ],
      "metadata": {
        "id": "_1479r70mrpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_set, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_set, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "rtAC4Az4mt_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "su9ZT7eRDP5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertQuestionClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_dim=768, device=None):\n",
        "    super(BertQuestionClassifier, self).__init__()\n",
        "\n",
        "    if device is None:\n",
        "      self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    else:\n",
        "      self.device = device\n",
        "\n",
        "    self.bert = BertModel.from_pretrained('bert-base-uncased').to(self.device)\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(hidden_dim, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      # nn.Dropout(0.1),\n",
        "      nn.Linear(hidden_dim, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      # nn.Dropout(0.1),\n",
        "      nn.Linear(hidden_dim, 2),\n",
        "    ).to(self.device)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask=None, labels=None, token_type_ids=None):\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    sequence_output = outputs[1]\n",
        "    logits = self.classifier(sequence_output)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "4br6ea85mwqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertQuestionClassifier()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
      ],
      "metadata": {
        "id": "jt6kwBNimyoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.bert.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "0SytZacWm04f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_single_epoch():\n",
        "  n_examples = len(train_set)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  total_accuracy = 0\n",
        "  for i ,batch in tqdm(enumerate(train_dataloader)):\n",
        "    #print([v[0] for k,v in batch.items()])\n",
        "    batch = {k: v.to(model.device).squeeze(1) for k,v in batch.items()}\n",
        "    logits = model(**batch)\n",
        "    loss = criterion(logits, batch[\"labels\"])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    total_loss += loss.sum().item()\n",
        "    total_accuracy += (logits.argmax(dim=1) == batch[\"labels\"]).sum().item()\n",
        "  return total_loss / n_examples, total_accuracy / n_examples\n",
        "\n",
        "\n",
        "def eval_once():\n",
        "  n_examples = len(val_set)\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "  total_accuracy = 0\n",
        "  for i ,batch in tqdm(enumerate(val_dataloader)):\n",
        "    batch = {k: v.to(model.device).squeeze(1) for k,v in batch.items()}\n",
        "    logits = model(**batch)\n",
        "    loss = criterion(logits, batch[\"labels\"])\n",
        "    total_loss += loss.sum().item()\n",
        "    total_accuracy += (logits.argmax(dim=1) == batch[\"labels\"]).sum().item()\n",
        "  return total_loss / n_examples, total_accuracy / n_examples\n"
      ],
      "metadata": {
        "id": "j6c3Ry8Lm25B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "NNbnwWXfBXff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in batch.items():\n",
        "  v = torch.vstack(v).transpose(1,0)\n",
        "  print(k, v.shape)"
      ],
      "metadata": {
        "id": "_RZM4zHeBdOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_acc = 0.0\n",
        "for epoch in range(100):\n",
        "  print(epoch, \"train (loss, acc)\", train_single_epoch())\n",
        "  print(epoch, \"val (loss, acc)\", eval_once())"
      ],
      "metadata": {
        "id": "_g9gNsUGm55d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}